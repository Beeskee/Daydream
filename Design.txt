Goals:

* good for automating Windows and Windows programs

  It should be good for automating installation, upgrading, uninstallation, and reconfiguration of software.  These are among the worst problems for Windows system administrators.

  It should be able to run arbitrary code when a button or button combination is pressed, globally or only when a certain window is active.  Infrequently used key combinations can be used to launch programs.  It is useful for adding some automation while using a particular program.  For example, quickly filling in fields in a badly-designed database interface.  It is useful for video games which did not bother to provide rapid fire.

  It should be good for remapping buttons, globally or only when a certain window is active.  This is useful for temporarily changing the keyboard layout.  For example, a computer technician who has switched to the Dvorak keyboard layout to treat their repetitive strain injury could use it while repairing the computer of someone that uses the QWERTY keyboard layout.  It is useful for programs that did not bother to provide proper button remapping.  3-D modelers and video games frequently do not provide button remapping, and may have a very awkward default mapping, especially when alternative keyboard layouts are used.

* good error handling

  Good error handling makes writing safe code and debugging easier.

  Where possible, the programming language should make errors impossible, without reducing what can be expressed in it.

  Detecting errors should require no effort.

  The damage errors can cause should be minimized.

  If the program does not handle an error, the programming language implementation should immediately halt execution, display what expectation was violated, where the error occurred, and the relevant values.

  Errors should be classified such that errors due to defects are separate from input/output errors.  Only input/output errors should normally be handled.  This prevents defects from being hidden by accident.

  It should be possible, though normally discouraged, to handle all errors.  This makes writing interpreters possible.

  Mechanism should be separated from policy.  Code that can experience errors should provide one or more ways to recover from those errors (mechanisms).  Code that calls code that can experience errors should be what chooses the way to handle an error (the policy).  This eliminates the need to hard-code the policy, a high-level concern, in low-level code, while allowing mechanisms to make use of state that is only present near the point where an error occurs.

* avoid surprising behavior

  Surprising behavior is the mother of all defects.

* consistent

  Consistency means not having to learn, remember, and abstract over, differences that should not exist.

  Constructs should always compose with any construct that can satisfy its requirements.

  Related constructs should have similar names.

  Related functions and macros should have similar parameters, and parameter order.

* avoid conflation

  Conflation means having to learn, remember, and abstract over, similarities that should not exist.  For example, many vector (array) operations only have useful semantics when sequential integer indexes are assumed, and therefore vectors and dictionaries should not be the same type.

  Constructs should never compose with any construct that cannot satisfy its requirements.

  Unrelated constructs should have different names.

* elegant

  Elegance usually results in less code to write, test, document, and maintain.

  * simple

    The number of primitive constructs should be minimized.

  * general

    Most constructs should be useful for many different purposes.

  * composable

    It should be possible to combine most constructs to construct new, useful, constructs.  The number of non-composable constructs should be minimized.

  * brief

    The amount of code should be minimized.  This refers to the number of tokens, not the token length.

  * relevant

    The majority of the code should relate to solving the problem.  The amount of code for input/output and resource management should be minimized.

  * seek symmetry

    Where practical, inverse functions should be provided.  Other forms of symmetry may also be of practical value.

* avoid unnecessary restrictions

  If the programming language is too restrictive, it will be unpleasant to work in.

  As much as practical should be implemented in the programming language itself.  If it is too restrictive, this will reveal it.

  It should be possible to implement a metacircular evaluator in the programming language, with a small amount of code, without much loss in efficiency.  This assures the programming language's implementation is exposed in a useful way to programs written in it.


Non-Goals:

* familiarity or ease of learning by novice programmers

  The syntax used in Lisp-like languages is not familiar to the average person.  Writing Lisp source code is difficult without an editor that helps with matching parentheses, and only Lisp programmers are likely to be familiar with one.  Many concepts, like closures, are also unfamiliar.  This makes Lisp-like languages more difficult to learn than most popular programming languages, because there is more to learn.

  The same features that make Lisp-like languages difficult to learn are what makes them superior to popular programming languages.  The syntax makes macros practical.  It makes macro calls look and act like built-in constructs without a lot of effort on the part of the macro author.  If macro authors had to avoid conflicting with syntax reserved for built-in constructs, and define custom mixfix syntax for their macros, few would bother.  Fully parenthesized syntax prevents defects due to forgetting operator precedence and associativity.  The unfamiliar concepts make it easier to build abstractions that are difficult to express in other programming languages.  For example, object-oriented programming support is usually built into popular programming languages, but is usually derived from mutable closures in Lisp-like languages.

  When a designer must choose whether to favor novices or experts, favoring experts is almost always the correct decision.  You are only a novice once, and spend more time as an expert than a novice.

  However, I do not intend to go out of my way to make it difficult to learn for novice programmers.

* compatibility with any existing Lisp-like languages

  Being compatible with an existing Lisp-like language means accepting all of their design decisions.  I am unaware of any Lisp-like languages where I agree with all of their design decisions.

  I am unaware of any Lisp-like languages that are intended for automating other programs.  It would not be surprising if one that was made different design decisions.

  If I am going to go to the trouble of writing my own Lisp-like language implementation, I am going to try to improve upon existing designs as much as possible in the process.

* syntactic innovation

  I hope to take most syntax from existing Lisp-like languages.  I want to keep the amount of punctuation very low.  I /like/ the simple, regular, appearance of Lisp source code.

  I may use some original names if I cannot find acceptable existing names.  I want frequently used constructs to have very short, but meaningful, names, so expressions remain readable.

  It is better for built-in constructs to have abbreviated names than user-defined constructs.  Programmers are more likely to know the meaning of the abbreviation for a built-in construct, due to having encountered it before.

  Abbreviating the names of built-in constructs makes the longer names available for programmers to use.  This is the most beneficial for constructors.  For example, Lisp programmers frequently want to name a variable "list", but that is usually the name of a list constructor.

  I will try to follow the conventions for abbreviating names in Lisp.  They are usually the first syllable.  They do not usually omit internal vowels, unlike abbreviated names in C.  Names consisting of multiple abbreviated names do not usually have hyphens separating them, and usually remove repeated letters (e.g. define function is "defn").  Popular abbreviations may override these rules, however.  For example, the abbreviation for define is "def", function is "fn", and list is "lst".

* parentheses reduction

  Parentheses shaming (e.g. "Lisp stands for Lots of Irritating Superfluous Parentheses") has inspired the authors of several Lisp-like languages (e.g. Arc and Clojure) to remove parentheses anywhere they could be inferred (e.g. around key-value pairs in a dictionary).

  This reduces the utility of structure editors and source code formatting tools, since parentheses inference is not possible in the general case (e.g. in macro calls).

  It makes source code difficult to understand, because the human has to infer the parentheses to understand the grouping.

  Lisp programmers should not mutilate their programming language in an attempt to please programmers that reject Lisp syntax.  They will not like it any more with less parentheses.  They will only be satisfied when it looks like C.

* raw strings

  Raw strings are only useful for Windows paths and regular expressions.

  This programming language uses "/" as the path delimiter, even on Windows.

  SREs are the best solution for regular expressions.  I plan to support them.

* dictionary and vector literals

  Having dictionary and vector literals raises questions about whether their elements should be evaluated or not, and causes defects by programmers incorrectly assuming they are or are not.

  It is necessary to use 'constructor syntax' (e.g. "(list x y z)") for lists to prevent ambiguity in pattern matching.  Otherwise it is impossible to tell the difference between a pattern construct and trying to bind a variable with the same name as a pattern construct.

  While the ambiguity problem is limited to lists, using a different syntax in pattern matching for everything but lists is not consistent.

  Having two syntaxes for the same thing is not simple.

  Dictionaries and vectors are shown as the function calls that would create them.

* reader macros

  Reader macros do not seem useful to me.  Without the ability to create new types and new human-readable representations there is little motivation for new literal syntax.

* overcomplicated macros

  Common Lisp's macro system implicitly captures variables.

  Scheme hygienic macro systems usually begin with destroying homoiconicity.  Requiring macros to work with syntax objects instead of normal values prevents the use of normal functions in macro expansion, which defeats the purpose of using a Lisp macro system, or requires the syntax objects to be converted to normal values, which defeats the purpose of using syntax objects.

  Some Scheme hygienic macro systems are limited to templates that are impractical to use for processing.

  Other Scheme hygienic macro systems are practical to use for processing, but include runaway complexity (e.g. phase separation).

  I have attempted to design a macro system that merely solves the problems with Common Lisp's macro system.

* mandatory static type checking

  Programmers often worry too much about whether their programming language is dynamically or statically type checked, and do not worry enough about whether their code is very dynamic or static.  The more general code is, the more dynamic it must be, because it must handle more possibilities at run time.  The more safe and efficient code is, the more static it must be, because it must prevent more possibilities at run time.  Since generality, safety, and efficiency are all desirable properties, the question should be which is more important for your code's intended use, not which is always superior.  It is easier to write dynamic code with dynamic type checking, and static code with static type checking, but either kind of code can be written with either kind of type checking.

  A mandatory static type system causes several problems.

  Static type systems are incompatible with exploratory programming.  They require you to specify, explicitly or implicitly, the type of a value before you know all of the operations that need to be performed with that value.  They are only useful for verifying known solutions.

  Programmers define an excess of types, and a small number of functions to work with each type, because using a separate type for each purpose minimizes the chance of operating on an invalid value.  It also minimizes code reuse.

  Type annotations make source code more verbose.  In theory, type inference, combined with a programming language design that assures type inference is always possible, can eliminate the need for type annotations.  In practice, even in programming languages with type inference, most definitions in the outermost scope must have type annotations, because most will be exported, and code in other modules would break if their type changed.

  Making changes requires more effort, because you must change the type annotations everywhere a value is used when you want to change its type.

  Generic static type systems often have poor payoff per effort ratios for improving safety and efficiency.

  Simple static type systems are not sufficient to prove code correct.  Dependent type systems are sufficient, but require heavy annotation.  It is often easier to write a program to derive correct efficient code from your specification than to annotate correct handwritten code enough to satisfy a dependent type system.

  A static type system is not necessary nor sufficient to eliminate run time type checks.  Most variables in code written in dynamically type checked languages are used with a single type of value, and compilers can use type inference to eliminate most run time type checks.  Lisp compilers have been doing that for decades.  When sum types (a.k.a. tagged unions) are used in statically type checked languages, run time type checks (reading the tag) determine what code to run.

  What features should be included in a static type system?  Most are primarily or exclusively concerned with type specifiers; what type a value is.  I rarely make mistakes involving operating on the wrong type of value, and when I do it is almost always easy to detect and correct.  Making mistakes with units of measure is an exception.  I consider unit and dimensional analysis, especially with safe type coercion, very helpful.  I would prefer a static type system to be more concerned with type qualifiers; what qualities a value has.  For example, inferring and checking the immutability of variables and values, taint and trademark checking, and typestate checking.  Mistakes involving those are almost always difficult to detect and correct.  Forcing side effects to be clustered near execution entry points, by preventing referentially transparent procedures from calling non-referentially transparent procedures (which would make them not referentially transparent), results in designs where most procedures compose, and the code is easier to maintain and optimize.  Something like checked exceptions, but for a condition system, is probably the right way to assure input/output errors are handled.  Whatever you decide, if you include a mandatory static type system, everyone that uses your programming language is forced to learn and use it, even if it does not help solve their problems.  Trying to solve everyone's problems in a mandatory static type system results in runaway complexity.

  For all these reasons, and more, a dynamically type checked language with a Lisp macro system is "the right thing".  Macros can describe any static type system you want, if you want one.  Static type checking becomes optional and pluggable.  Functions written with different type systems can call each other with no effort.

* introduce a null type

  Do not confuse this with Scheme's terminology for the empty list.  The empty list will still exist in this programming language, and will be called "null".

  The absence of an equivalent to the null type in most Lisp-like languages results in abusing other types or values as sentinel values.  The empty list is the most common choice, but false is usually chosen when an empty list could be confused with a meaningful value.  This feels like a kludge, but it is the cleanest solution.

  If lists are described in terms of null references, the length of "'(null)" becomes ambiguous, because it is equivalent to "'()".  List operations lose useful semantics.  Converting vectors with trailing nulls to lists will result in lists that have less elements than the corresponding vector.  "cons"ing a value to a list does not necessarily result in a longer list, since that value might have been null.

  This does raise the question of what the "first" and "rest" of the empty list is.  In this programming language they are defects, as in Scheme and most Lisp-like languages other than Common Lisp (where they are the empty list).  This is consistent with trying to access a vector out of bounds.  The "rest" of a 1 element list is the empty list, and "null?" can be used to test for that.

* implement Scheme's numerical tower

  This would be nice, but it is a lot of work.

  While system administration may involve complex data structures (it certainly involves a lot of parsing), it rarely involves complex numerical math.

  I may support it eventually.

* mandatory referential transparency

  A programming language that enforces global referential transparency can:
  * Lie.  For example, claim to only generate a sequence of actions, and that the runtime performs the side effects the actions instruct it to.  If you believe that, then C also enforces global referential transparency in exactly the same way.  The actions are system calls.  The runtime is the operating system kernel and drivers.  It just has very convenient syntax for the IO and state monads.
  * Be limited to behaving like a fancy calculator, where all variables and data structures are immutable, and no input/output is possible.  Code written in a more capable programming language could display the return values of the non-interactive programs.

  Since I want this programming language to be as useful as possible, I want it to be able to perform input/output.

* mandatory immutability

  Functions that mutate their local variables, but not their arguments or free variables, are as composable and maintainable as those written in programming languages with mandatory immutability, and are sometimes shorter, easier to understand, and more efficient.

  At least one free variable /must/ be mutated to share state between event handlers.

* object-oriented programming

  I believe object-oriented programming usually does more harm than good.  Mutable global variables make code difficult to maintain.  When a global variable becomes corrupt, it is difficult to tell what code was responsible.  Mutable objects are mutable global variables.  Implementation inheritance makes code difficult to maintain.  Changing a method's implementation can break subtypes, because subtypes' implementations may depend on side effects of supertypes' methods.  Of course programmers can limit themselves to using immutable objects, and inheriting interfaces and using composition, but then objects are no improvement over records or dictionaries.

  This programming language has mutable closures, and it should be possible to implement a CLOS-like system using them, which is more powerful than popular programming languages' object-oriented programming support, but I do not plan to write that myself.

  That does imply that this programming language will have a fixed set of types.  It is better to have a small number of types and a large number of procedures that operate on each, than to have a large number of types and a small number of procedures that operate on each, because more code can be reused.  Dictionaries are used in place of new types.

* transparent (as in Smalltalk) or non-transparent (as in Lisp Machine Lisp) persistence

  Transparent persistence is a marvelous feature, so long as you do not try to use it to replace the file system (as Smalltalk did).  Hardware and software are likely to fail eventually, and if that occurs it is important to be able to recover programs and data, which is almost impossible if they are only stored in a, likely corrupted, core dump.

  Unfortunately, it is very difficult to implement, and very inefficient, if you do not implement your own allocator.  Running on top of AutoHotkey makes that impossible.

  If this programming language was implemented in a programming language with manual memory management, I would support this feature.

* implicit control flow

  Declarative programming is said to be specifying what to do, not how to do it.  In practice you always have to specify how to do it, because there are too many possibilities, and most are too inefficient or would have negative side effects.

  Most programming languages considered to be declarative have implicit control flow, which allows you to avoid specifying how to do control flow, so long as the implicit control flow is what you need.

  Some people consider functional programming languages with strict evaluation to be declarative, but they require specifying how to do everything like imperative programming languages.  Programming languages with lazy- or speculative evaluation, or goal-directed evaluation (like Icon), are more deserving of the title, but still require specifying how to do most control flow.  Good examples include constraint, pull- and push dataflow, deductive-, inductive-, and abductive logic, and string-, term-, and graph rewriting programming languages.

  Which implicit control flow algorithm should be included?  Each one is useful in some situations.  Whatever you decide, if you include an implicit control flow algorithm, everyone that uses your programming language is forced to learn and use it, even if it does not help solve their problems.

  When the implicit control flow is /not/ what is needed, it requires a lot of effort to work around it.

  Other than push dataflow, implicit control flow is unsafe to use with code that has side effects.  It becomes difficult to determine if, or in what order, each side effect will occur.

  Implicit control flow makes it difficult to predict the time and space efficiency of code.  Optimizing compilers for programming languages with implicit control flow spend a lot of effort working around it.

  For all these reasons, and more, a strictly evaluated language with a Lisp macro system is "the right thing".  Macros can describe any implicit control flow algorithm you want, if you want one.  Implicit control flow becomes optional and pluggable.  Functions written with different implicit control flow algorithms can call each other with no effort.

* overcomplicated pattern matching

  I will limit pattern matching to destructuring and tests for equality in patterns, arbitrary test expressions in "when" guards, and dispatching to code associated with a pattern.  New pattern matching features will not be added unless they help achieve the goals I set for this programming language.

  A discussion of pattern matching features that have been rejected follows.

  Pattern guards, like those in Haskell, are qualifiers (expressions) that any in-scope values can be passed to before pattern matching.  If the pattern match on the value of the qualifier succeeds, then its bindings can be used by qualifiers that follow, else qualifiers and patterns that follow are skipped, and pattern matching proceeds with the next equation (sequence of qualifiers and patterns).

  Views, like those in Haskell, are similar, but less powerful.  They are equivalent to a qualifier preprocessing a single value before pattern matching.

  Pattern guards and views encourage bad code.  If you could use them, it probably indicates that your data structure or function decomposition needs to change.  If you usually need to preprocess values from a data structure in the same way, the data structure is the problem.  The data structure should be changed to match the parameters of the functions that process the data.  If you usually need to preprocess values from a data structure in different ways, the function decomposition is the problem.  The preprocessing code should be moved into separate functions, and the return values of those functions should be used as the arguments of the functions that process the data.

  The only advantage pattern guards have (views have none) over fixing the code is short-circuit evaluation.  That is an optimization for a situation that is too rare to justify its inclusion in a programming language.

  Variable-length patterns, like those in Racket, are redundant.  Processing data structures of variable-depth or -length is best done with recursion.  Recursion is more general, because it can be used for more than pattern matching.  Racket uses variable-length patterns to construct, and sometimes filter, lists and bind them.  Pattern matching should be used for destructuring, not structuring (constructing data structures) or restructuring (transforming data structures).  Structuring and restructuring is best done in the code associated with a pattern.

  Some of Racket's pattern matching features are made redundant by "when" guards, which Racket also supports.  It includes a pattern construct that matches if a function passed the value returns true, which is equivalent to putting the same function call in the "when" guard.  It includes a pattern construct that matches if a regular expression matches the string value, which is equivalent to putting the same test in the "when" guard.  "when" guards are more general, because they can perform tests involving multiple values.

* comprehensions

  Comprehensions are inspired by set-builder notation from mathematics.

  Comprehensions conflate "filter" and "map".

  To avoid the construction and traversal of intermediate data structures, use streams.

* transducers

  Transducers are transformations from one reducing (folding) function to another.

  Transducers are unnecessarily restrictive, since they are limited to one order of traversal (usually "foldl1").

  They cause surprising behavior in a programming language with partial application, because it is ambiguous whether functions that can return transducers return a transducer or partially applied function when they are passed less than the maximum number of arguments.

  To avoid the construction and traversal of intermediate data structures, use streams.

* parallelism

  AutoHotkey does not support multithreading, making this difficult to implement.

  To this day most C and C++ code is not thread-safe, so many programming languages with easy integration with C and C++ do not support multithreading.  AutoHotkey may never do so.

  There are hacks abusing inter-process communication for this purpose in the forums, similar to Python's multiprocessing and concurrent.futures implementations.  I may eventually add support for thread pools and futures that way.

* networking

  AutoHotkey does not support networking, making this difficult to implement.

  There are some libraries that use DLL calls for this purpose in the forums.  I may eventually add support for networking that way.

* high security

  There is not much point in having sandboxing, taint or trademark checking, or similar security features when the programming language does not support networking.

  I think sandboxing would be practical to implement.  Definitions from the "unsafe" module would not be imported into untrusted code's modules, and untrusted code would be restricted to importing a user-defined set of definitions.  It would still be trivial to crash the interpreter by exhausting memory, but the same can be said of most sandbox implementations.  It would be sufficient to keep untrusted code from performing harmful input/output.

  I will consider implementing sandboxing after networking is implemented.

  I do not expect security features to be relevant to most uses of this programming language.  It is not intended for writing web apps.

* high efficiency

  Running on top of AutoHotkey makes high efficiency impossible.  It was never designed with efficiency in mind.  It uses string interpolation of code, which is roughly equivalent to parsing lines repeatedly.  It uses reference counting, which we cannot avoid even on objects that have to be traced to be garbage collected (e.g. environments).  It makes no attempt at optimization.  Source code written on one line, using commas to separate statements, is up to 35% faster than properly formatted source code, for some unfathomable reason.

  So long as the implementation is not too wasteful it should still be usable.

  While I do not plan to write an optimizing compiler for this programming language, I have attempted to avoid design decisions that would make that more difficult.


Noteworthy Features:

* intended use

  This programming language is designed to be easy to embed in another programming language, to be used for event handling, and make it easy to call foreign functions.

* good error handling and exploratory programming

  This programming language uses different syntax for initialization and mutation to better detect defects.  Redefining a variable normally indicates the programmer did not know it was in use.  Redefinition can occur via definition or importing.  Mutating a variable that has not been defined indicates the programmer typoed the name.

  In this programming language mutating an imported variable is normally a defect, because it should not normally be necessary to violate encapsulation.

  However, being unable to correct a defective definition interferes with exploratory programming.  For this reason redefinition and mutating imported variables works at the listener.  A warning is issued in case it was accidental.  Mutating a variable that has not been defined is still a defect at the listener.

  Be aware that redefinition and mutation are not the same.  Redefinition affects the value in the current module.  Mutation affects the value in the home module, which is not the same as the current module for imported variables.

  Modules can also be reloaded to correct defective definitions.

* modernization

  Lisp parachronisms have been replaced or removed, since we use IBM PC compatibles instead of IBM 704s, and monitors instead of teleprinters.  "car" is replaced with "first", "cdr" is replaced with "rest", and "car" and "cdr" compositions (like "caddar") have been removed.  You "show" things on the monitor, not "print" them.

* syntax

  The syntax for Booleans ("true" and "false") and integers in bases other than 10 ("0b101" and "0xF00") is similar to popular programming languages.

  "true" and "false" are /values/, not variables.  To avoid surprising behavior, "true" and "false" are not valid variable names.

* definition

  Definition is much simpler than in most Lisp-like languages.  There is only one "def" form, which can be used in any scope.  There are no "let" forms.  "let" forms are no improvement over using "fn" or "mac" directly.

* documentation

  Docstrings can be associated with modules, variables in the outermost scope, functions, and macros.  Functions are provided for searching for combinations of patterns in module names, variable names (including parameters), and docstrings, types, and values.  While this was common in the past, modern Lisp-like languages seem to be becoming more static, and less useful interactively, thus losing one of their advantages over other programming languages.  Functions are provided to load all code in a directory, or in a directory hierarchy, to make searching easier.

* symbols

  To avoid surprising behavior, the same name in the same scope always refers to the same value, barring mutation.  This implies that variables and functions are not in separate namespaces.  It implies that the programming language is case-insensitive.  To prevent the use of symbols that jangle like bad typography, uppercase letters in symbols are a defect.

  To avoid surprising behavior, a symbol always looks like a symbol.  Any characters that have special meaning to the parser, such as whitespace or parentheses, cannot be included in a symbol.

* types

  Types are rarely converted implicitly, and never when it could be surprising.  This is typical of Lisp-like languages, but surprising type coercion is typical of most programming languages.

  Only "true" and "false" are true and false.  Integers can be converted to (0 is false, anything else is true) and from (true is 1, false is 0) Booleans, explicitly.  This should make debugging easier, and I have always found treating non-Boolean values as Boolean surprising.

  Data structures can be called with an argument to access elements.  Like 'normal' functions they map their domain to their range.  Dictionaries map keys to values.  Lists map the symbols "first" and "rest" to values.  Strings map 0-based integer indexes to single-character strings.  Vectors map 0-based integer indexes to values.  This is more consistent and brief than most Lisp-like programming languages.

  Improper lists are not supported.  The second argument to "cons" must be a list.  Improper lists seem to be inferior to proper lists or vectors for all purposes, and the infix syntax they use is unlike everything else in Lisp.

  There is no character type.  Strings are effectively immutable vectors of strings of 1 character.  Strings can be indexed like a vector.  Strings of 1 character can be converted to and from integers.  This may result in more code reuse.

  Vectors are dynamic arrays.  Values can be 'pushed' to, or 'popped' from, the highest index, allowing them to double as stacks.  Singly-linked lists support efficient pushing to, and popping from, the beginning, but not efficient indexing.  This makes vectors more versatile, and since bounds are still checked, they are still safe.  Like singly-linked lists, dynamic arrays are used in most hash table implementations, which are used in most environment implementations, so it makes sense to expose them in the programming language.  It requires little effort and is consistent with the goal of not unnecessarily restricting the programming language.

* built-ins

  The built-ins for manipulating the interpreter, and the foreign function interface, which is used to perform all input/output, are defined in the "unsafe" module.  All other built-ins are defined in the "base" module.  All definitions in these modules are normally imported into every other module implicitly, so it is possible for programmers to define things.

  While redefining a variable in a module is normally a defect, built-ins can be redefined with only a warning.  This allows new built-ins to be defined without breaking existing code.

  The built-ins are as general as possible.  For example, there is only one "map" for all sequence collection types (list, string, and vector).  To avoid surprising type coercion, the collection returned is of the same type as the collection argument.  This should allow more code reuse, and minimize what programmers must learn and remember to use the programming language.

* generalized variables

  The "set!" special form accepts any mutable place:
  * variables
  * a dictionary and key
  * a list and "'first" or "'rest"
  * a vector and index

  This is much simpler than in Common Lisp.

  For example:
  "(set! (a-dict "x") (a-dict "y"))"

* equality

  Equality is much simpler than in most Lisp-like languages.  "=" compares values for immutable types, and identities (i.e. addresses) for mutable types, like "==" in most popular programming languages.  Dictionaries always use this form of equality, because it assures keys remain associated with their values.  "equal?" compares values even for mutable types, and correctly handles reference cycles, like "equal?" in Scheme.  Pattern matching always uses this form of equality, because it handles immutable and mutable types consistently.

* regular expressions

  SREs (s-expression regular expressions) are supported.  They may be missing features the host programming language's regular expression implementation does not support, since I plan to reuse that.  I do not plan to support features the host programming language does, but Schs's SREs do not.

* control flow

  * sequence

    "do" evaluates a sequence of forms in order and returns the value of the last expression.  An empty "do" is a defect, because the value is uninitialized.  Control flow sequences are implicit in most places where you might want to use them, except for "if".  "do" can be used by macros to generate multiple definitions and expressions in the same scope, since it does not create a new scope.

  * branch

    * if

      One-armed "if" is a defect, because the value is uninitialized when the test expression is false.  Equivalents which return "null" when the test expression is false, "when" and "unless", are supported for side effects.  Programmers are often wrong when they believe nothing needs to be done when the test expression is false.  "when" and "unless" act as warning signs of potential defects.

    * cond

      Non-exhaustive "cond" clauses is a defect, because the value is uninitialized when all test expressions are false.  This implies an empty "cond" is a defect.

    * pattern matching

      Pattern matching is used for destructuring.  The features supported are very similar to OCaml's.  The syntax is very similar to Racket's.

      Some aspects of OCaml's, Racket's, and this programming language's pattern matching are the same, but should still be mentioned.

      Non-exhaustive patterns is a defect, because the value is uninitialized when no patterns match.

      Not matching every element of a data structure is a defect, because allowing that would prevent defining patterns that match data structures with different numbers of elements.

      Not binding the same variables in every pattern in an "or" pattern construct is a defect, because the values of the variables are uninitialized when a pattern matches that does not bind them.

      Pattern constructs can be nested.  While recursive functions could be used instead, the inability to describe patterns more than one level deep would prevent a common usage of pattern matching from working; pattern matching function arguments.  You could put the arguments into a data structure to pattern match, but then you could not destructure their contents.  You could pattern match the arguments individually, but then you could not easily dispatch to the appropriate code for the combination of pattern matches.

      Some aspects of this programming language's pattern matching differs from OCaml's, Racket's, or both, and should be mentioned.

      Racket's non-linear pattern matching is supported.  If an identifier is used multiple times within a pattern, the corresponding matches must be the same according to "equal?", except that instances of an identifier in different "or" sub-patterns are independent.

      The Haskell pattern matching extension record puns is supported for dictionaries.  It uses syntax like "("x")" to bind variables with the same name as string or symbol keys for the associated keys.  Using string keys that do not contain valid symbol names with this feature is a defect.

      Racket's "cons" pattern construct refers to mutable cons cells in this programming language.  This programming language only has mutable cons cells.

      Racket's "and" pattern construct is supported.  It is more general than the "as" pattern construct in other programming languages.  It is useful for binding "or" and "inr" pattern matches, in addition to destructuring and equality testing.

      OCaml's closed interval pattern construct is supported, and named "inr".  It has two parameters, the first for the low bound, the second for the high bound.  It is more convenient than specifying each element of a closed interval of numbers or single-character strings in an "or" pattern construct.

      "when" guards use s-expression prefix syntax, since this programming language does not support keyword parameters.  They cannot appear inside a pattern, because all variables must be bound before a "when" guard can be evaluated.  They are useful for testing individual variables for something more complex than equality, or testing relationships between variables.

* functions

  * scope

    Only lexical scope is supported.  Dynamic scope prevents composition and makes code difficult to maintain.  Closures and partial application are a composable, maintainable, alternative to dynamic scope for avoiding repeatedly specifying an argument.

  * calls

    While (trailing) optional and rest parameters are supported, most forms are of fixed-arity.  Partial application only works with fixed-arity forms.  If a form cannot be used with partial application (e.g. special forms that do not evaluate all their arguments in order and macros), and variable-arity could be useful, it is used.  Most constructors (e.g. "dict", "lst", and "vec") are variadic, because if they were not it would be impossible to create data structures with different numbers of elements.  "do" (explicit and implicit) is variadic, because if it was not it would be impossible to create control flow sequences with different numbers of expressions.  Optional parameters follow "?", while the rest parameter follows "&".  To avoid surprising behavior, "?" and "&" are not valid variable names.  Keyword parameters are not supported.  They cannot be used with partial application, and interact in surprising ways with optional and rest parameters.  Data structures are often used where optional, rest, and keyword parameters would be used in other Lisp-like languages.

    Function arguments are evaluated once, from left to right, as in Common Lisp.  It is bad style to depend on that, but it seems better to standardize it than to have it be implementation-defined as it is in so many programming languages.  Optimizing compilers may evaluate arguments without side effects however they please.

    Partial application is supported.  If you call a fixed-arity function with less than the number of arguments it expects, it will return a closure that will accept the remaining arguments and has the closed-over arguments in its environment.  Parameters should appear in order from the most to least likely to be reused.  This makes using closures much more convenient.  Calling a fixed-arity function with the expected number of arguments causes it to run normally.  Calling a function with too many arguments is a defect.

    "fix-arity" creates a fixed-arity copy of a variadic form to make it easier to build closures with.

    If you want to omit values in the middle of a function's arguments, instead of only at the end, you can use "_" in their place.  The resulting closure will expect the remaining arguments in the same order.  To avoid surprising behavior, "_" is not a valid variable name.

    Function parameters can be rearranged to ease closure creation like so "#(bad-fn %2 %1)".  The rest parameter can be passed with "%&".  To avoid surprising behavior, "%" followed by an integer and "%&" are not valid variable names.

    Recursion is properly supported.  Call stack frames are heap allocated, so non-tail recursion will succeed unless memory is exhausted.  Tail call elimination is performed, so loops can be defined recursively.

  * continuations

    Delimited continuations and dynamic-wind are supported.  Undelimited continuations are not.  Delimited continuations compose.  Undelimited continuations do not.

    The delimited continuations will be similar to the ones in Racket.

  * returns

    Everything returns a value.  Forms that perform definition or mutation return the value that they used.  Forms that involve a control flow sequence return the value returned by the last expression in the control flow sequence.  This, along with combining allocation and initialization, prevents uninitialized values.

    Multiple return values are not supported.  Programming languages that allow them must provide constructs to bind them, which are less general than destructuring constructs.  Evaluation should reduce a tree of expressions to a single value.  Pattern matching makes binding multiple values inside a data structure trivial.

* quoting

  "quote", "quasiquote", "unquote", and "unquote-splicing" work like they do in Common Lisp (e.g. when nested).

  "unquote" or "unquote-splicing" outside of "quasiquote" is a defect.

  Destructive unquote-splicing is not supported, since it seems to be only useful for causing defects.

* macros

  * semantics

    * source code transformation

      This macro system transforms data that is equivalent to source code.  Passing an argument to a macro, or returning a value from a macro, that contains a type that has no literal syntax is a defect.  Passing an argument to a macro, or returning a value from a macro, that contains a list with a reference cycle is a defect.

      If a macro's expansion contains a module-qualified symbol, and it does not refer to a variable in the outermost scope of the macro's home module, it is a defect.  Only variables in the outermost scope are always defined.

      If a macro is called outside its home module, its expansion contains a module-qualified symbol, and it does not refer to an exported variable, it is a defect.  This restriction prevents dependence on implementation details.  The variable does not have to be exported from the macro's home module.  The variable only has to be exported from the variable's home module, which may not be the same as the macro's home module.  If modules had to import all variables their macro calls use, module interfaces would be dependent on implementation details of the macros they use.  If modules had to export all variables their macro expansions use, module interfaces would be cluttered with constructs implemented elsewhere (e.g. most modules would reexport most of the "base" and "unsafe" modules).  This encourages encapsulation.

    * hygiene

      The macro hygiene problem is due to variables not referring to the right places.

      Problems with accidental capture come in two varieties:
      * code generated by a macro may capture variables used in other code
      * other code may capture variables used in code generated by a macro

      This macro system is based on the idea of automatically using Common Lisp's solutions to these problems ("gensym" and packages, correspondingly), with their flaws corrected.

      The macro system uses several types of symbols:
      * name
      * gensymed
      * module-qualified
      * captured
      * literal

      The parser and "sym" function generate name symbols.

      Name symbols may be coerced to a gensymed or module-qualified symbol as appropriate when returned from a macro.

      The values associated with symbols returned from a macro are compared to "def", "fn", and "mac" when determining how name symbols are used.  This is an equality test for identity (i.e. address), so they are correctly recognized when rebound.  Name symbols are looked up in the macro's environment.  Gensymed symbols are not looked up (that would be unnecessary).  Module-qualified symbols are looked up in the associated module's outermost scope.  There may be no value associated with some symbols (e.g. symbols for variables in the outermost scope, parameters, and local variables).

      If the name symbol is used as a variable in the outermost scope, it is not coerced when returned from a macro.

      If the name symbol is used as a parameter or a local variable, it is coerced to a gensymed symbol when returned from a macro.

      Gensymed symbols construct unique places for each gensymed symbol for each macro expansion.  This avoids surprising behavior by preventing name collisions.  They are effectively symbols named by a global counter.  They are unique due to forbidding symbols in source code from having integer names, and incrementing the global counter by 1 on each use.  They are still shown with a valid symbol name preceding the integer.

      Otherwise, the name symbol is coerced to a module-qualified symbol when returned from a macro.

      Module-qualified symbols separate the places in the macro definition and call environments.  This avoids surprising behavior by preventing rebinding from affecting code generated by macros.  Module-qualified symbols refer to variables in the outermost scope of the macro's home module (where it was defined).

      This is sufficient for the expansion of hygienic macros.

      Anaphoric macros require the ability to capture variables.

      The "cap" function converts name symbols contained in its argument, which can be any data equivalent to source code, to captured symbols.

      Calling "cap" at run time is a defect.

      Captured symbols are coerced to name symbols when returned from a macro.

      This is sufficient for the expansion of anaphoric macros.

      Occasionally symbols are not used as identifiers (e.g. in computer algebra systems).  These symbols should not be transformed by the macro system.

      The "lit" function converts name symbols contained in its argument, which can be any data equivalent to source code, to literal symbols.

      Calling "lit" at run time is a defect.

      Literal symbols are coerced to name symbols when transitioning from macro expansion time to run time.

      This solution was chosen for several reasons:
      * Homoiconicity is preserved.  Captured and literal symbols support the name symbol interface.  Macros are able to pass values to and from normal functions with no effort.
      * Macros can easily perform processing and side effects.
      * The macro system is not much more complex than Common Lisp's.  Most of the additional complexity is in implementation, not use.  Common Lisp already had name, gensymed, and module-qualified symbols, though with somewhat different semantics.  Those are still the only types of symbols that exist at run time.
      * In the common case, when writing hygienic macros where all symbols are identifiers, the programmer does not have to provide extra specification.  When writing anaphoric macros, or macros where some symbols are not identifiers, the programmer has to provide less specification than Common Lisp's macro system, and far less specification than Scheme's syntax-case macro system.
      * The gensym counter is not observable outside each module without effort on the part of the programmer.  This avoids interfering with incremental compilation.

    * symbol macros

      Symbol macros are supported.  A symbol macro call looks like a symbol.  They are useful as generalized variables.

      They are constructed with "symac".  "symac"'s first parameter is a symbol.  Symbol macros have no parameters.

      Name symbols are not coerced when returned from a symbol macro.  This reduces the specification necessary when constructing them.

      Symbol macros are like other macros in all other respects.

    * evaluation

      Macros differ from functions in that their arguments are not evaluated before their body, and they execute before run time.

      If macros' arguments were evaluated before their body, as in function application, macros could not inspect, transform, or selectively evaluate their arguments.  Using a macro with function application (e.g. with "apply", folds, "map", or "filter") is a defect.

      If macros did not execute before run time, they would be less useful.  They could not be used for static analysis or optimization.  They could not be used for extending the language in ways that are easy to statically analyze or optimize.  Executing before run time, and reducing the extended language to the base language (the language without macros), is what makes them superior to fexprs.  Calling a macro at run time is a defect.

      Macros are like functions in all other respects.  They can be constructed inside arbitrary expressions, be defined locally, use free variables, call functions or macros, perform input/output, be passed as arguments to functions, and be returned from functions.  Be aware that, like functions, macros cannot be passed as arguments to macros, or be returned from macros, because they do not correspond to source code.  Symbols for functions and macros can be passed as arguments to macros, or be returned from macros, however.

  * explicit macro expansion

    This programming language provides several special forms to perform macro expansion explicitly.  They can be used to observe a macro expansion.  They can be used by macros that inspect or transform constructs.

    "(expand-defs form)" recursively expands macro calls in the form when the macros are defined (not imported) in the current module.  This is primarily useful to reduce clutter when trying to understand your own macro expansions.

    "(expand-once form)" expands macro calls in the form once.  If the form is not a macro call, but contains macro calls, each macro call will be expanded once.  This is primarily useful when trying to understand recursive macro expansions.

    "(expand-show form symbols)" recursively expands macro calls in the form when their symbol names match the symbol names in the list.  This is primarily useful to reduce clutter when trying to understand specific macro expansions.

    "(expand-hide form symbols)" recursively expands macro calls in the form unless their symbol names match the symbol names in the list.  This is primarily useful to reduce clutter caused by specific macro expansions when trying to understand other macro expansions.

    "(expand-when form function)" recursively expands macro calls in the form when the predicate function returns true.  The function parameters are "(symbol args)" for the macro call's symbol and arguments.  This is primarily useful when writing code walking macros that inspect or transform constructs (including macro calls).

    These special forms use the current environment to determine the value associated with a symbol, like a macro call.  That is usually the expected and desired behavior.

  * local macro definition

    "def" defines variables in the current scope.  While that is usually the expected and desired behavior, it may not be obvious how to define a local macro to simplify definitions in the outermost scope.

    You could construct a macro in the outermost scope, with your local macro definition inside it, and a body which would return a "do" form containing your definitions in the outermost scope, then call it immediately by surrounding the "mac" form in an additional pair of parentheses.  The advantage is this does precisely what you want.  The disadvantage is this is complicated.

    You could define the macro used to simplify definitions in the outermost scope, in the outermost scope, but not export it.  The advantage is this is simple.  The disadvantage is this pollutes the module's outermost scope.

* conditions

  There is a condition system that is similar to the one in Common Lisp.  It is used to handle defects, input/output errors, and warnings.  Using it as a general-purpose communication mechanism is discouraged, because invisible communication channels make code difficult to maintain.  A condition system seems to be the only way to satisfy our error handling design goals.

  Warnings are issued when code needs maintenance but still works.  For example, it might use a depreciated construct, or a bad practice that was previously tolerated.  Warnings are shown by default, since that is the only responsible way to design a programming language.  History has shown most programmers make no effort to find problems in their code.  Condition handlers can choose other ways to handle warnings, such as logging or discarding them.

  Sometimes the condition system cannot report the exact location of a problem, because the code with the problem was generated instead of parsed.  In these cases it reports the closest location possible, which is usually an "eval" or macro call.  That is usually the expected and desired behavior.

* modules

  I have attempted to keep the module system as simple as possible while still enforcing good modularity.

  The module system serves multiple purposes:
  * libraries                              - supports code reuse
  * swapping implementations of interfaces - supports maintenance

  The module system features solve practical problems:
  * selective import - prevents namespace pollution
  * renaming imports - resolves name collisions
  * selective export - prevents dependence on implementation details

  A module effectively encapsulates part of the outermost scope.  Only one instance of a module exists at a time, which is observable via mutation.  Modules do not nest, but they do compose.

  The module system works with /variables/, not values, in the outermost scope.  If an exported variable is mutated, other modules that import it can read the current value.

  Modules do not export anything implicitly.  The programmer must explicitly specify the interface.  This encourages encapsulation, planning interfaces, and conscious interface change.

  A module can have any number of import- and export specifications mingled with definitions and expressions.  Import- and export specifications are cumulative.  They never remove an import or export.  The only restriction on order is that variables must be initialized by definition or importing before they are read.  This makes macro use easier.

  Import- and export specifications inside definitions or expressions, except for (potentially nested) "do" forms, is a defect.  Module interfaces should not vary.  Macros may expand into import- and export specifications, which is sufficient to allow module implementations to vary at macro expansion time.  Import- and export specifications are allowed in "do" forms so a single macro call can expand into a combination of import- and export specifications, definitions, and expressions.  The value of an import- or export specification is "null".

  Dependency cycles are a defect.  They indicate two or more modules are badly decomposed.  Some definitions need to be combined into a single module to eliminate the cycle.  Others probably need to be separated into other modules.

  All state, except for the abstract syntax tree, is discarded after each module is macro expanded.  The abstract syntax tree is reevaluated as needed to create the necessary state for the macro expansion of other modules or run time.  This prevents the state within a module at macro expansion time from influencing the macro expansion of other modules, or from influencing run time.

  There is a one-to-one relationship between files and modules.

  Modules may be versioned.  Unversioned modules are usually used to ease maintenance.  Versioned modules are usually used for code reuse.  A versioned module's file name contains the module name, followed by a hyphen, then the version in Semantic Versioning format.  A version number consists of non-negative integers for major-, minor-, and patch numbers, in that order, separated by periods.  The major version number is 0 before release, and incremented to 1 at release, then incremented by 1 each time backwards incompatible changes are made.  The minor version number is incremented by 1 each time features are added or depreciated.  The patch version number is incremented by 1 each time defects are corrected.  When the previous number is incremented by 1, the following numbers are reset to 0 (e.g. 1.2.15 to 1.3.0).

  Modules are imported from by specifying a relative path and file name, and what you want to import.  Only forward path traversal is allowed (i.e. "." and ".." are not allowed), to discourage aliasing.  The programming language attempts to look up the module relative to the program's working directory, but if that fails it tries a central repository.  If the module is versioned, the major version number must be = the requested number, and the minor and patch version numbers must be >= the requested numbers.  The requested patch version number should be 0 unless the programmer knows their implementation is affected by a defect that is corrected in a specific patch version.  The programming language will use the most recent version that matches the requirements.

  This module system could support incremental compilation (with the abstract syntax trees corresponding to binaries, and reevaluation corresponding to reloading the binaries), but I chose it due to the lack of surprising behavior.

  Modules can be reloaded, both at the listener and in modules, to update their definitions.  This is useful for exploratory programming, or hot swapping to correct a defect in, or add features to, a running program.  Be aware that this only updates the values of the imported variables.  References to out-of-date values may still exist.  You should therefore prefer to access imported values through the variables you imported them into.

* garbage collector

  The garbage collector is continuous, generational, and incremental (i.e. marking and sweeping are broken down into multiple steps).  This should result in low, and fairly steady, latency.

  While, past a point, a trade-off must be made between low-latency and high-throughput, generational garbage collection improves both.

  Favoring low-latency over high-throughput seems like the right decision for interactive programs, like those most likely to be written in this programming language.

  It is possible to disable garbage collection, perform increments only when the program is idle, and force non-incremental full garbage collections.  These features are often desired by those writing software intended for use in short bursts or video games.  AutoHotkey scripts are often used in short bursts, and often used with video games.

  In all cases a non-incremental full garbage collection will still occur if memory is almost exhausted, because a pause is less disruptive than crashing.

  The interface is inspired by Lua's garbage collector.
