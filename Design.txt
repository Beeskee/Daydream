Goals

* good error handling

  Good error handling makes writing safe code and debugging easier.

  When possible, the programming language should make errors impossible without reducing what can be expressed in it.

  Detecting errors should require no effort.

  The damage errors can cause should be minimized.

  If the program does not handle an error, the runtime system should immediately halt execution and show what expectation was violated, where the error occurred, and the relevant values.

  Errors should be classified such that errors caused by defects are separate from input/output errors.  Only input/output errors should normally be handled.  This prevents defects from being hidden by accident.

  It should be possible, though normally discouraged, to handle all errors.  This makes writing interpreters possible.

  Mechanism should be separated from policy.  Code that can experience errors should provide one or more ways to recover from those errors (mechanisms).  Code that calls code that can experience errors should choose the way to handle an error (the policy).  This eliminates the need to hardcode the policy, a high-level concern, in low-level code while making it possible for mechanisms to use state that is only present near the point where an error occurs.

* avoid surprising behavior

  Surprising behavior is the mother of all defects.

* consistent

  Consistency means not having to learn, remember, and abstract over differences that should not exist.

  Constructs should always compose with constructs that can satisfy their requirements.

  Similar constructs should have similar names.

  Similar functions and macros should have similar parameters and parameter order.

* avoid conflation

  Conflation means having to learn, remember, and abstract over similarities that should not exist.  For example, many vector (array) operations only have useful semantics when sequential integer indexes are assumed; therefore, vectors and dictionaries should not be the same type.

  Constructs should never compose with constructs that cannot satisfy their requirements.

  Dissimilar constructs should have different names.

* elegant

  Elegance usually results in less code to write, test, document, and maintain.

  * simple

    The number of primitive constructs should be minimized.

  * general

    Most constructs should be useful for many different purposes.

  * composable

    It should be possible to combine most constructs to construct new, useful constructs.  The number of non-composable constructs should be minimized.

  * brief

    The amount of code should be minimized.  This refers to the number of tokens, not the token length.

  * relevant

    The majority of the code should relate to solving the problem.  The amount of code for input/output and resource management should be minimized.

  * seek symmetry

    When practical, inverse functions should be provided.  Other forms of symmetry may also be of practical value.

* avoid unnecessary restrictions

  If the programming language is too restrictive, it will be unpleasant to use.

  As much as practical should be implemented in the programming language itself.  If it is too restrictive, this will reveal it.

  It should be possible to implement a metacircular evaluator in the programming language, without much code, without much loss in efficiency.  This ensures the programming language's implementation is exposed in a useful way to software written in it.

* beware irreversible design decisions

  When possible, avoid design decisions that make including new or changing existing features impossible without reimplementing the programming language or software written in it.

  Make irreversible design decisions consciously and cautiously.


Non-Goals

* familiarity or ease of learning by novice programmers

  The syntax used in Lisp-like languages is not familiar to the average person.  Writing Lisp source code is difficult without an editor that helps with matching parentheses, and only Lisp programmers are likely to be familiar with one.  Many concepts, like closures, are also unfamiliar.  This makes Lisp-like languages more difficult to learn than most popular programming languages because there is more to learn.

  The same features that make Lisp-like languages difficult to learn are what makes them superior to popular programming languages.  The syntax makes macros practical.  It makes macro calls look and act like built-in constructs without a lot of effort on the part of the macro author.  If macro authors had to avoid conflicting with syntax reserved for built-in constructs, and define custom mixfix syntax for their macros, few would bother.  Fully parenthesized syntax prevents defects caused by forgetting operator precedence and associativity.  The unfamiliar concepts make it easier to build abstractions that are difficult to express in other programming languages.  For example, object-oriented programming support is usually built into popular programming languages, but it is usually derived from mutable closures in Lisp-like languages.

  When a designer must choose whether to favor novices or experts, favoring experts is almost always the correct decision.  You normally spend more time as an expert than a novice.

  However, I do not intend to go out of my way to make it difficult to learn for novice programmers.

* compatibility with any existing programming languages

  Being compatible with an existing programming language means accepting /all/ of their design decisions.  I am unaware of any programming languages where I agree with all of their design decisions.

  If I am going to design my own programming language, I am going to try to improve upon existing designs as much as possible.

* implement Scheme's numerical tower

  This is not immediately useful to me.

  If I implement this, I would be likely to include dynamic unit and dimensional analysis.

  I may eventually implement this.

* syntactic innovation

  I hope to take most syntax from existing Lisp-like languages.  I want to keep the amount of punctuation very low.  I /like/ the simple, regular, appearance of Lisp source code.

  I may use some original names if I cannot find acceptable existing names.  I want frequently used constructs to have very short, but meaningful, names so that expressions remain readable.

  It is better for built-in constructs to have abbreviated names than user-defined constructs.  Programmers are likely to be familiar with built-in constructs.

  Abbreviating the names of built-in constructs makes the longer names available for programmers to use.  This is the most beneficial for constructors.  For example, Lisp programmers often want to name a variable "list", but that is usually the name of a list constructor.

  I will try to follow the conventions for abbreviating names in Lisp.  They are usually the first syllable.  They do not usually omit internal vowels, unlike abbreviated names in C.  Names consisting of multiple abbreviated names do not usually have hyphens separating them and usually remove repeated letters (e.g. define function is "defn").  Popular abbreviations sometimes violate these conventions, however (e.g. define is "def", function is "fn", and list is "lst").

* parentheses reduction

  Parentheses shaming (e.g. "Lisp stands for Lots of Irritating Superfluous Parentheses") has inspired the authors of several Lisp-like languages (e.g. Arc and Clojure) to remove parentheses anywhere they could be inferred (e.g. around key-value pairs in a dictionary).

  This reduces the utility of structure editors and source code formatting tools because parentheses inference is not possible in the general case (e.g. in macro calls).

  It makes source code difficult to understand because the human has to infer the parentheses to understand the grouping.

  Lisp programmers should not mutilate their programming language in an attempt to please programmers that reject Lisp syntax.  They will not like it any more with less parentheses.  They will only be satisfied when it looks like C.

* raw strings

  Raw strings are only useful for regular expressions and Windows paths.

  SREs are the best solution for regular expressions.  I plan to support them.

  This programming language uses "/" as the path delimiter, even on Windows.

* dictionary and vector literals

  Having dictionary and vector literals raises the question of whether their elements are evaluated or not and causes defects by programmers incorrectly assuming they are or are not.

  It is necessary to use 'constructor syntax' (e.g. "(lst x y z)") for lists to prevent ambiguity in pattern matching.  Otherwise, it is impossible to tell the difference between a pattern construct and trying to bind a variable with the same name as a pattern construct.  Although the ambiguity would be limited to lists, using a different syntax in pattern matching for everything but lists is not consistent.  Having two syntaxes for the same thing is not simple.

* reader macros

  Reader macros do not seem useful to me.  Without user-defined types and user-defined representations, there is little motivation for user-defined literal syntax.

* overcomplicated macros

  Common Lisp's macro system implicitly captures variables.

  Scheme hygienic macro systems usually begin with destroying homoiconicity.  Requiring macros to work with syntax objects, instead of normal values, prevents the use of normal functions in macro expansion, which defeats the purpose of using a Lisp macro system, or requires the syntax objects to be converted to normal values, which defeats the purpose of using syntax objects.

  Some Scheme hygienic macro systems are limited to templates that are impractical to use for processing.

  Other Scheme hygienic macro systems are practical to use for processing but include runaway complexity (e.g. phase separation).

  I have attempted to design a macro system that merely solves the problems with Common Lisp's macro system.

* object-oriented programming

  I believe object-oriented programming usually does more harm than good.  Mutable global variables make code difficult to maintain.  When a global variable becomes corrupt, it is difficult to tell what code was responsible.  Mutable objects are mutable global variables.  Implementation inheritance makes code difficult to maintain.  Changing a method's implementation can break subtypes because subtypes' implementations may depend on side effects of supertypes' methods.  Of course programmers can limit themselves to using immutable objects, and inheriting interfaces and using composition, but then objects are no improvement over records or dictionaries.

  This programming language has mutable closures, and it should be possible to implement a CLOS-like system using them, which is more powerful than popular programming languages' object-oriented programming support, but I do not plan to write that myself.

  That does imply that this programming language will have a fixed set of types.  It is better to have a small number of types and a large number of procedures that operate on each, than to have a large number of types and a small number of procedures that operate on each because more code can be reused.  Dictionaries are used in place of new types.

* mandatory static type checking

  Programmers often worry too much about whether their programming language is dynamically or statically type checked and do not worry enough about whether their code is very dynamic or static.  The more general code is, the more dynamic it must be because it must handle more possibilities at run-time.  The more safe and efficient code is, the more static it must be because it must prevent more possibilities at run-time.  Generality, safety, and efficiency are all desirable properties, so the question should be which is more important for your code's purpose, not which is always superior.  It is easier to write dynamic code with dynamic type checking and static code with static type checking, but either kind of code can be written with either kind of type checking.

  A mandatory static type system causes several problems.

  Static type systems are incompatible with exploratory programming.  They require you to specify, explicitly or implicitly, the type of a value before you know all of the operations that need to be performed with that value.  They are only useful for verifying known solutions.

  Programmers define an excess of types and a small number of functions to work with each type because using a separate type for each purpose minimizes the chance of operating on an invalid value.  It also minimizes code reuse.

  Type annotations make source code more verbose.  In theory, type inference, combined with a programming language design that ensures type inference is always possible, can eliminate the need for type annotations.  In practice, even in programming languages with type inference, most definitions in the outermost scope must have type annotations because most will be exported, and code in other modules would break if their type changed.

  Making changes requires more effort because you must change the type annotations everywhere a variable is used when you want to change its type.

  Generic static type systems often have poor payoff per effort ratios for improving safety and efficiency.

  Simple static type systems are not sufficient to prove code correct.  Dependent type systems are sufficient, but require heavy annotation.  It is often easier to write a program to derive correct efficient code from your specification than to annotate correct handwritten code enough to satisfy a dependent type system.

  A static type system is not necessary nor sufficient to eliminate run-time type checks.  Most variables in code written in dynamically type checked languages are used with a single type of value, and compilers can use type inference to eliminate most run-time type checks.  Lisp compilers have been doing that for decades.  When sum types (a.k.a. tagged unions) are used in statically type checked languages, run-time type checks (reading the tag) determine what code to run.

  What features should be included in a static type system?  Most are primarily or exclusively concerned with type specifiers: what type a value is.  I rarely make mistakes involving operating on the wrong type of value, and when I do it is almost always easy to detect and correct.  Making mistakes with units of measure is an exception.  I consider unit and dimensional analysis, especially with safe type coercion, very helpful.  I would prefer a static type system to be more concerned with type qualifiers: what qualities a value has.  For example, inferring and checking the immutability of variables and values, taint checking, and typestate checking.  Mistakes involving those are almost always difficult to detect and correct.  Forcing side effects to be clustered near execution entry and exit points by preventing referentially transparent procedures from calling non-referentially transparent procedures (which would make them not referentially transparent), results in designs where most procedures compose, the code is easier to maintain and optimize, and most procedures can be safely evaluated with immediate feedback.  Something like checked exceptions, but for a condition system, is probably the right way to ensure input/output errors are handled.  Whatever you decide, if you include a mandatory static type system, everyone that uses your programming language is forced to learn and use it, even if it does not help solve their problems.  Trying to solve everyone's problems in a mandatory static type system results in runaway complexity.

  For all these reasons and more, a dynamically type checked language with a Lisp macro system is "the right thing".  Macros can describe any static type system you want, if you want one.  Static type checking becomes optional and pluggable.  Functions written with different type systems can call each other with no effort.

* mandatory referential transparency

  A programming language that enforces global referential transparency can
  * Lie.  For example, claim to only generate a sequence of actions, and that the runtime system performs the side effects the actions instruct it to.  If you believe that, then C also enforces global referential transparency in exactly the same way.  The actions are system calls.  The runtime system is the operating system kernel and drivers.  It just has very convenient syntax for the IO and state monads.
  * Be limited to behaving like a fancy calculator, where all variables and data structures are immutable, and no input/output is possible.  Code written in a more capable programming language could show the return values of the non-interactive programs.

  I want to use this programming language for interactive programs.

* mandatory immutability

  Functions that mutate their local variables, but not their arguments or free variables, are as composable and maintainable as those written in programming languages with mandatory immutability, and are sometimes shorter, easier to understand, and more efficient.

  At least one free variable /must/ be mutated to share state between event handlers.

* transparent (as in Smalltalk) or non-transparent (as in Common Lisp) persistence

  Transparent persistence makes it possible to resume execution of a program as if nothing had gone wrong if a power or hardware failure (that does not corrupt data) occurs.

  Non-transparent persistence is no improvement over loading source code or binaries.

  Transparent persistence should not be used to replace the file system (as Smalltalk did).  It is important to be able to recover user data if a hardware failure corrupts data.  That is difficult if all system data, user data, and the contents of memory are amalgamated.

  The data used to resume execution is usually stored in an image file.  An image contains the set of all reachable data in a program (including the program counter and call stack frames).

  Image files are not just memory written to disk.  They may be versioned so that the image file format can be changed.  References may be stored in a relocatable format, like how the listener shows cyclical references.  Recovering from an image requires applying fix-ups to data associated with external resources, like files and network sockets.

  Images must be updated atomically.  Otherwise, a power or hardware failure during an update to the image would corrupt it.  If the operating system provides transactional file access, that can be used to safely update the image.  Otherwise, an updated image must be written to another file, then the original file must be replaced with the updated file.

  It must be possible to associate external resources with recovery procedures.  The recovery procedures are executed when recovering from an image.  In most cases a recovery procedure should put the datum into a state that will cause an input/output error condition when it is written or read.  That is appropriate for most uses of files and network sockets because the files may have changed and the network connections may have closed since the program terminated.  That is not appropriate for graphical user interfaces because they could not have changed since the program terminated.

  Several hooks would be useful for implementing transparent persistence.  The garbage collector could have a hook for allocating a datum and another for deallocating a datum.  Those could be used to tell the transparent persistence implementation when to save a datum to or delete a datum from the image.  The runtime system could have a hook for mutating a datum.  That could be used to tell the transparent persistence implementation when to change a value associated with a datum.  These hooks would be useful for implementing other things (e.g. space profilers; invariant inferencers; record, rewind, and replay debuggers).

  Transparent persistence requires integration with the garbage collector.

  Transparent persistence requires the host to have certain features to be efficient:
  * Parallelism.  Otherwise, allocation, mutation, and deallocation, which require updating the image, occur at disk write speed.
  * Transactional file access.  Otherwise, updating the image requires twice the time and space.

  Transparent persistence is not helpful if the host uses its own state recovery mechanism.  For example, a word processor might save the location of each document file, the location of the cursor in each document file, and the contents of each document file, when they change, and use this data to recover its state whenever it is executed.  If this programming language were embedded in the word processor just described (for scripting), transparent persistence would provide no additional benefit.

  Transparent persistence is /harmful/ if the host uses a flash drive to store the image.  The frequent writes will rapidly wear out the drive.

  Those are enough reasons to not require transparent persistence.

  If this programming language were implemented in a host without tracing garbage collection, and with the ability to support efficient transparent persistence, I would consider implementing it.

* implicit control flow

  In theory, declarative programming is specifying what to do, not how to do it.  In practice, you usually have to specify how to do it because there are too many possible solutions, and most are too inefficient or would have harmful side effects.

  Most programming languages considered to be declarative have implicit control flow, which makes it possible to avoid specifying how to do control flow when the implicit control flow is helpful.

  Some people consider functional programming languages with strict evaluation to be declarative, but they require specifying how to do everything like imperative programming languages.  Programming languages with lazy or speculative evaluation, or goal-directed evaluation (like Icon), are more deserving of the title, but still require specifying how to do most control flow.  Good examples include answer set; constraint; pull and push dataflow; deductive, inductive, and abductive logic; production system; and string, term, and graph rewriting programming languages.

  Which implicit control flow algorithm should be included?  Each one is useful in some situations.  Whatever you decide, if you include an implicit control flow algorithm, everyone that uses your programming language is forced to learn and use it, even if it does not help solve their problems.

  When the implicit control flow is /harmful/, a lot of effort is required to work around it.

  Implicit control flow makes it difficult to predict the time and space efficiency of code.  Optimizing compilers for programming languages with implicit control flow spend a lot of effort working around it.

  Except for push dataflow, implicit control flow is unsafe to use with side effects.  It becomes difficult to determine if, or in what order, each side effect will occur.

  For all these reasons and more, a strictly evaluated language with a Lisp macro system is "the right thing".  Macros can describe any implicit control flow algorithm you want, if you want one.  Implicit control flow becomes optional and pluggable.  Functions written with different implicit control flow algorithms can call each other with no effort.

* overcomplicated pattern matching

  I will limit pattern matching to destructuring and tests for equality in patterns, arbitrary test expressions in "when" guards, and dispatching to code associated with a pattern.  New pattern matching features will not be included unless they help achieve the goals I set for this programming language.

  A discussion of pattern matching features that have been rejected follows.

  Pattern guards, like those in Haskell, are qualifiers (expressions) that any in-scope values can be passed to before pattern matching.  If the pattern match on the value of the qualifier succeeds, then its bindings can be used by qualifiers that follow, else qualifiers and patterns that follow are skipped, and pattern matching proceeds with the next equation (sequence of qualifiers and patterns).

  Views, like those in Haskell, are similar, but less powerful.  They are equivalent to a qualifier preprocessing a single value before pattern matching.

  Pattern guards and views encourage bad code.  If you could use them, it probably indicates that your data structure or function decomposition needs to change.  If you usually need to preprocess values from a data structure in the same way, the data structure is the problem.  The data structure should be changed to match the parameters of the functions that process the data.  If you usually need to preprocess values from a data structure in different ways, the function decomposition is the problem.  The preprocessing code should be moved into separate functions, and the return values of those functions should be used as the arguments of the functions that process the data.

  The only advantage pattern guards have (views have none) over fixing the code is short-circuit evaluation.  That is an optimization for a situation that is too rare to justify its inclusion in a programming language.

  Variable-length patterns, like those in Racket, are redundant.  Processing data structures of variable depth or length is best done with recursion.  Recursion is more general because it can be used for more than pattern matching.  Racket uses variable-length patterns to construct, and sometimes filter, lists and bind them.  Pattern matching should be used for destructuring, not structuring (constructing data structures) or restructuring (transforming data structures).  Structuring and restructuring is best done in the code associated with a pattern.

  Some of Racket's pattern matching features are made redundant by "when" guards, which Racket also supports.  It includes a pattern construct that matches if a function passed the value returns "true", which is equivalent to putting the same function call in the "when" guard.  It includes a pattern construct that matches if a regular expression matches the string value, which is equivalent to putting the same test in the "when" guard.  "when" guards are more general because they can perform tests involving multiple values.

* comprehensions

  Comprehensions are inspired by set-builder notation from mathematics.

  Comprehensions conflate "filter" and "map".

  To avoid the construction and traversal of intermediate data structures, use streams.

* transducers

  Transducers are transformations from one reducing (folding) function to another.

  Transducers are unnecessarily restrictive because they are limited to one order of traversal (usually "foldl1").

  Transducers cause surprising behavior in a programming language with partial application because it is ambiguous whether functions that can return transducers return a transducer or partially applied function when they are passed less than the maximum number of arguments.

  To avoid the construction and traversal of intermediate data structures, use streams.

* parallelism

  Some hosts do not support parallelism.

  To this day most code is not thread-safe.  This programming language performs input/output by using foreign code.

  I am unlikely to ever implement this.

* efficiency

  Some host programming languages are inefficient.

  So long as the implementation is not too wasteful, it should still be usable.

  Although I do not plan to implement an optimizing compiler for this programming language, I have attempted to avoid design decisions that would make that more difficult.

  If code needs to be very efficient, it should be written for the Vulkan API.  Hosts can expose that to this programming language.

* standardized input/output facilities

  The input/output facilities of hosts vary.


Noteworthy Features

* purpose

  This programming language is designed to be embedded in other programs.

* modernization

  Lisp parachronisms have been replaced or removed because we use IBM PC compatibles instead of IBM 704s and monitors instead of teleprinters.  "car" is replaced with "first", "cdr" is replaced with "rest", and "car" and "cdr" compositions (like "caddar") have been removed.  You "show" things on the monitor, not "print" them.  You do exploratory programming at the listener, not the REPL.

* documentation

  Docstrings can be associated with modules, variables in the outermost scope, functions, and macros.  Functions are provided for searching for combinations of patterns in module names, variable names (including parameters), and docstrings, types, and values.  Although this was common in the past, modern Lisp-like languages seem to be becoming more static, and less useful interactively, thus losing one of their advantages over other programming languages.  Functions are provided to load all code in a directory, or in a directory hierarchy, to make searching easier.

* symbols

  To avoid surprising behavior, the same name in the same scope always refers to the same value, barring mutation.  This implies that variables and functions are not in separate namespaces and the programming language is case-insensitive.  To prevent the use of symbols that jangle like bad typography, uppercase letters in a symbol is a defect.

  To avoid surprising behavior, a symbol always looks like a symbol.  Any characters that have special meaning to the parser, such as whitespace or parentheses, cannot be included in a symbol.

* literals

  The syntax for Booleans ("true" and "false") and integers in bases other than 10 ("0b101" and "0xF00") is similar to popular programming languages.

  "true" and "false" are /values/, not variables.  To avoid surprising behavior, "true" and "false" are not valid variable names.

* types

  Types are rarely converted implicitly and never when it could be surprising.  This is typical of Lisp-like languages, but surprising type coercion is typical of most programming languages.

  Only "true" and "false" are true and false.  Integers can be converted to ("0" is "false", anything else is "true") and from ("false" is "0", "true" is "1") Booleans, explicitly.  This should make debugging easier, and I have always found treating non-Boolean values as Boolean surprising.

  Data structures can be applied to arguments to access elements.  Like 'normal' functions they map their domain to their range.  Dictionaries map keys to values.  Lists and strings map the symbols "first" and "rest" to values.  Vectors map 0-based integer indexes to values.  This is more consistent and brief than most Lisp-like languages.

  Improper lists are not supported.  Any operation that would produce one is a defect.  Improper lists seem to be inferior to proper lists or vectors for all purposes, and the infix syntax used to specify them is inconsistent with the rest of Lisp.

  There is no character type.  A string is effectively an immutable list of strings of 1 character.  This may result in more code reuse.  Strings cannot be mutated because they are frequently used as hash table keys.  Strings cannot be indexed because UTF-8 is a variable-length code.  Strings of 1 character can be converted to and from integers.

  Vectors are dynamic arrays.  Vectors may be used as stacks.  This makes vectors more general.  Bounds are checked.  Like singly-linked lists, dynamic arrays are used in most hash table implementations, which are used in most environment implementations.  Exposing them to programs written in the programming language avoids unnecessarily restrictions.

* definition and mutation

  Definition and mutation are different special forms so that this programming language can detect defects caused by typos in variable names.

  Definition is much simpler than in most Lisp-like languages.  There is only one "def" form which can be used in any scope.

  Defining the same variable as different values (according to "eqv?") is normally a defect so that this programming language can detect accidental redefinition.  Redundant definitions are /not/ defects so that macros that generate definitions are easier to use.  Optimizing compilers may eliminate redundant definitions that do not cause observable side effects.

  Mutating an undefined mutable place is a defect.

* generalized variables

  Mutation is much simpler than in most Lisp-like languages.  There is only one "set!" form which can be used with any mutable place.

  A mutable place can be specified by
  * a variable
  * a dictionary and key
  * a list and "'first" or "'rest"
  * a vector and index

  Examples
  "(set! x 0)"
  "(set! (dictionary 'x) 0)"
  "(set! (list 'first) 0)"
  "(set! (vector 0) 0)"

* equality

  Equality is much simpler than in most Lisp-like languages.  "eqv?" compares values for immutable types and identities (i.e. memory addresses) for mutable types, like "==" in most popular programming languages.  Dictionaries always use this form of equality because it ensures keys remain associated with their values.  "equal?" compares values even for mutable types and correctly handles reference cycles, like "equal?" in Scheme.  Pattern matching always uses this form of equality because it handles immutable and mutable types consistently.

* regular expressions

  SREs (s-expression regular expressions) are supported.  They may be missing features the host programming language's regular expression implementation does not support because I plan to reuse that.  I do not plan to support features the host programming language does, but Schs's SREs do not.

* control flow

  * sequence

    "do" evaluates a sequence of forms in order and returns the value of the last expression.  An empty "do" is a defect because the value is uninitialized.  Control flow sequences are implicit in most places where you might want to use them, except for "if".  "do" can be used by macros to generate multiple expressions in the same scope because it does not create a new scope.

  * branch

    * if

      One-armed "if" is a defect because the value is uninitialized when the predicate returns "false".  Equivalents which return "null" when the predicate returns "false", "when" and "unless", are supported for side effects.  Programmers are often wrong when they believe nothing needs to be done when the predicate returns "false".  "when" and "unless" act as warning signs of potential defects.

    * cond

      Non-exhaustive "cond" clauses is a defect because the value is uninitialized when all test expressions are false.  This implies an empty "cond" is a defect.

    * pattern matching

      Pattern matching is used for destructuring.  The features supported are very similar to OCaml's.  The syntax is very similar to Racket's.

      Some aspects of OCaml's, Racket's, and this programming language's pattern matching are the same, but should still be mentioned.

      Non-exhaustive patterns is a defect because the value is uninitialized when no patterns match.

      Not matching every element of a data structure is a defect because that would make it impossible to use a pattern to match a data structure with a specific number of elements.

      Not binding the same variables in every pattern in an "or" pattern construct is a defect because the values of the variables are uninitialized when a pattern matches that does not bind them.

      Pattern constructs can be nested.  Whereas recursive functions could be used instead, the inability to describe patterns more than one level deep would prevent a common usage of pattern matching from working: pattern matching function arguments.  You could put the arguments into a data structure to pattern match, but then you could not destructure their contents.  You could pattern match the arguments individually, but then you could not easily dispatch to the appropriate code for the combination of pattern matches.

      Some aspects of this programming language's pattern matching differs from OCaml's, Racket's, or both, and should be mentioned.

      Racket's non-linear pattern matching is supported.  If an identifier is used multiple times within a pattern, the corresponding matches must be the same according to "equal?", except that instances of an identifier in different "or" sub-patterns are independent.

      The Haskell pattern matching extension record puns is supported for dictionaries.  It uses syntax like "("x")" to bind variables with the same name as string or symbol keys for the associated keys.  Using string keys that do not contain valid symbol names with this feature is a defect.

      Racket's "cons" pattern construct refers to mutable cons cells in this programming language.  This programming language only has mutable cons cells.

      Racket's "and" pattern construct is supported.  It is more general than the "as" pattern construct in other programming languages.  It is useful for binding "or" and "inr" pattern matches, as well as destructuring and equality testing.

      OCaml's closed interval pattern construct is supported and named "inr".  It has two parameters, the first for the low bound, the second for the high bound.  It is more convenient than specifying each element of a closed interval of numbers or single-character strings in an "or" pattern construct.

      "when" guards use s-expression prefix syntax because this programming language does not support keyword parameters.  They cannot appear inside a pattern because all variables must be bound before a "when" guard can be evaluated.  They are useful for testing individual variables for something more complex than equality, or testing relationships between variables.

    * functions and macros

      * scope

        Only lexical scope is supported.  Dynamic scope prevents composition and makes code difficult to maintain.  Closures and partial application are a composable, maintainable alternative to dynamic scope for avoiding repeatedly specifying an argument.

      * calls

        Even though (trailing) optional and rest parameters are supported, most forms are of fixed-arity.  Partial application only works with fixed-arity functions.  If a form cannot be used with partial application (special forms and macros), and variable-arity could be useful, it is used.  Most constructors (e.g. "dict", "lst", and "vctr") are variadic because if they were not it would be impossible to create data structures with different numbers of elements.  "do" (explicit and implicit) is variadic because if it were not it would be impossible to create control flow sequences with different numbers of expressions.  Optional parameters follow "?", whereas the rest parameter follows "&".  To avoid surprising behavior, "?" and "&" are not valid variable names.  Keyword parameters are not supported.  They cannot be used with partial application, and interact in surprising ways with optional and rest parameters.  Data structures are often used where optional, rest, and keyword parameters would be used in other Lisp-like languages.

        Function arguments are evaluated once, from left to right, as in Common Lisp.  It is bad style to depend on that, but it seems better to standardize it than to have it be implementation-defined as it is in so many programming languages.  Optimizing compilers may generate code that evaluates arguments that do not cause observable side effects in any order.

        Partial application is used where "apply" would be used in other Lisp-like languages.  If you call a fixed-arity function with less than the number of arguments it expects, it will return a closure that will accept the remaining arguments and has the closed-over arguments in its environment.  Parameters should appear in order from the most to least likely to be reused.  This makes using closures much more convenient.  Calling a fixed-arity function with the expected number of arguments causes it to run.  Calling a function with too many arguments is a defect.

        "fix-arity" creates a fixed-arity copy of a variadic function to make it easier to build closures with.

        If you want to omit values in the middle of a function's arguments, instead of only at the end, you can use "_" in their place.  The resulting closure will expect the remaining arguments in the same order.  To avoid surprising behavior, "_" is not a valid variable name.

        Function parameters can be rearranged to ease closure creation like so "#(bad-fn %2 %1)".  The rest parameter can be passed with "%&".  To avoid surprising behavior, "%" followed by an integer and "%&" are not valid variable names.

        Recursion is properly supported.  Call stack frames are heap allocated so that non-tail recursion will succeed unless memory is exhausted.  Tail call elimination is performed so that loops can be defined recursively.

      * continuations

        Delimited continuations and dynamic-wind are supported.  Undelimited continuations are not.  Delimited continuations compose.  Undelimited continuations do not.

        The delimited continuations will be similar to the ones in Racket.

      * returns

        Everything returns a value.  Forms that perform definition or mutation return "null".  Forms that involve a control flow sequence return the value returned by the last expression in the control flow sequence.  This, along with combining allocation and initialization, prevents uninitialized values.

        Multiple return values are not supported.  Programming languages that support them must provide constructs to bind them, which are less general than destructuring constructs.  Evaluation should reduce a tree of expressions to a single value.  Pattern matching makes binding multiple values inside a data structure trivial.

* quoting

  "quote", "quasiquote", "unquote", and "unquote-splicing" work like they do in Common Lisp (e.g. when nested).

  "unquote" or "unquote-splicing" outside of "quasiquote" is a defect.

  Destructive unquote-splicing is not supported because it seems to be only useful for causing defects.

* macros

  * semantics

    * source code transformation

      This macro system transforms data that is equivalent to source code.

      Passing an argument to or returning a value from a macro that contains a type that has no literal syntax is a defect.

      Passing an argument to or returning a value from a macro that contains a list with a reference cycle is a defect.

      If any module-qualified symbols in the macro's expansion do not refer to a variable in the outermost scope of the macro's home module (where the macro was defined), it is a defect.  Only variables in the outermost scope are always defined.

      If a macro is called outside its home module and any module-qualified symbols in the macro's expansion do not refer to an exported variable, it is a defect.  This prevents dependence on implementation details.  The variable does not have to be exported from the macro's home module.  The variable only has to be exported from the variable's home module, which may not be the same as the macro's home module.  If modules had to import all variables their macro calls use, module interfaces would be dependent on implementation details of the macros they use.  If modules had to export all variables their macro expansions use, module interfaces would be cluttered with constructs implemented elsewhere (e.g. most modules would reexport most of the "core" module).

    * hygiene

      The macro hygiene problem is caused by variables not referring to the right places.

      Problems with accidental capture come in two varieties:
      * macro-generated code may capture variables used in other code
      * other code may capture variables used in macro-generated code

      This macro system is based on the idea of automatically using Common Lisp's solutions ("gensym" and packages, correspondingly) with their flaws corrected.

      The macro system uses several types of symbols:
      * name
      * gensymed
      * module-qualified
      * captured

      The parser and "sym" function generate name symbols.

      A name symbol at the beginning of a list that is coerced to a module-qualified symbol with the value of "quote", "quasiquote", "unquote", "unquote-splicing", "def", or "fn" causes some name symbols that follow to be handled differently.  This is a test for identity so that they are correctly recognized when rebound.

      If the name symbol is quoted or bound in the outermost scope, it is not coerced when returned from a macro.

      If the name symbol is bound in an inner scope, all name symbols with the same name in the same scope are coerced to a gensymed symbol with the same counter value when returned from a macro.

      Gensymed symbols construct unique places, for each gensymed symbol, for each macro expansion.  This avoids surprising behavior by preventing name collisions.  They are effectively symbols named by a global counter.  They are unique because symbols in source code are forbidden from following the gensymed symbol naming convention and the global counter is incremented by 1 after each gensymed symbol is generated.

      Otherwise, the name symbol is coerced to a module-qualified symbol qualified with the macro's home module when returned from a macro.

      Module-qualified symbols separate the places in the macro definition and call environments.  This avoids surprising behavior by preventing rebinding from affecting macro-generated code.  They are symbols whose value is looked up in the outermost scope of the module they are qualified with.

      This is sufficient for the expansion of hygienic macros.

      Anaphoric macros require the ability to capture variables.

      The "cap" function converts unquoted name symbols in its argument, which can be any data equivalent to source code, to captured symbols.

      Captured symbols are coerced to name symbols when returned from a macro.

      Calling "cap" at run-time, outside of a listener or sandbox, is a defect.  Captured symbols only have useful semantics at macro-expansion-time.

      This is sufficient for the expansion of anaphoric macros.

      This solution was chosen for several reasons:
      * Homoiconicity is preserved.  All symbol types support the name symbol interface.  Macros are able to pass values to and from normal functions with no effort.
      * Macros can easily perform processing and side effects.
      * This macro system is not much more complex than Common Lisp's.  The increased complexity is in implementation, not use.  Common Lisp already had name, gensymed, and module-qualified symbols, though with somewhat different semantics.  Those are still the only types of symbols that exist after macro expansion.
      * Macros are briefer in this macro system than in Common Lisp's or Scheme's.  Hygienic macros are briefer than anaphoric macros.
      * The gensym counter is not observable outside each module without effort on the part of the programmer.  This avoids interfering with incremental compilation.

    * symbol macros

      A symbol macro call appears to be a symbol.  They are useful as generalized variables.

      They are constructed with "symac".  "symac"'s first argument is a symbol.  Symbol macros have no parameters.

      Symbol macros are like other macros in all other respects.

    * evaluation

      Macros differ from functions in that their arguments are not evaluated before their body and macros execute before run-time.

      If macro arguments were evaluated before their body, as in function application, macros could not inspect or transform their arguments.  Using a macro with function application (e.g. with folds, "map", or "filter") is a defect.

      If macros did not execute before run-time, they would be less useful.  They could not be used for static analysis or optimization.  They could not be used for extending the language in ways that are easy to statically analyze or optimize.  Executing before run-time and reducing the extended language to the base language (the language without macros) is what makes them superior to fexprs.  Calling a macro at run-time, outside of a listener or sandbox, is a defect.  Optimizing compilers may eliminate macros before run-time.

      Macros are like functions in all other respects.  They can be constructed inside arbitrary expressions, be defined locally, use free variables, call functions or macros (including themselves), perform input/output, and be passed as arguments to and be returned from functions.  Be aware that, like functions, macros cannot be passed as arguments to or be returned from macros because they are not equivalent to source code.  Symbols for functions and macros can be passed as arguments to or be returned from macros, however.

  * explicit macro expansion

    This programming language provides several functions to perform macro expansion explicitly.  They can be used to observe a macro expansion.  They can be used by macros that inspect or transform constructs.

    "(expand-defs form)" recursively expands macro calls in the form when the macros are defined (not imported) in the current module.  This is primarily useful to reduce clutter when trying to understand your own macro expansions.

    "(expand-once form)" expands macro calls in the form once.  If the form is not a macro call but contains macro calls, each macro call will be expanded once.  This is primarily useful when trying to understand recursive macro expansions.

    "(expand-show form symbols)" recursively expands macro calls in the form when their symbol names match the symbol names in the sequence.  This is primarily useful to reduce clutter when trying to understand specific macro expansions.

    "(expand-hide form symbols)" recursively expands macro calls in the form unless their symbol names match the symbol names in the sequence.  This is primarily useful to reduce clutter caused by specific macro expansions when trying to understand other macro expansions.

    "(expand-when form predicate)" recursively expands macro calls in the form when the predicate returns "true".  The predicate function parameters are "(symbol args)" for the macro call's symbol and arguments.  This is primarily useful when writing code walking macros that inspect or transform constructs (including macro calls).

    These functions use the current environment to determine the value associated with a symbol, like a macro call.  That is usually the expected and desired behavior.

    Calling these functions at run-time, outside of a listener or sandbox, is a defect.  Macros only have useful semantics at macro-expansion-time.

  * local macro definition

    It may not be obvious how to define a local macro to simplify definitions in the outermost scope because "def" defines variables in the current scope.

    You could construct the macro in the outermost scope then call it immediately by surrounding the "mac" form and any arguments in parentheses (like "let" for macros).  The advantage is this does precisely what you want.  The disadvantage is this is complicated.

    You could define the macro in the outermost scope and not export it.  The advantage is this is simple.  The disadvantage is this pollutes the module's outermost scope.

* conditions

  There is a condition system that is similar to the one in Common Lisp.  It is used to handle defects, input/output errors, and warnings.  Using it as a general-purpose communication mechanism is discouraged because invisible communication channels make code difficult to maintain.  A condition system seems to be the only way to satisfy our error handling design goals.

  Conditions are arranged in an extensible type hierarchy to make it possible to handle similar conditions the same way.

  Warnings are issued when code needs maintenance but still works.  For example, it might use a depreciated construct or a bad practice that was previously tolerated.  Warnings are shown by default because that is the only responsible way to design a programming language.  History has shown most programmers make no effort to find problems in their code.  Condition handlers can choose other ways to handle warnings, such as logging or discarding them.

  Sometimes the condition system cannot report the exact location of a problem because the code with the problem was generated instead of parsed.  In these cases, it reports the closest location possible, which is usually a macro or sandboxed evaluator call.  That is usually the expected and desired behavior.

  There are useful built-in restarts for all built-in defect and input/output error types.

* modules

  * purpose

    The module system serves multiple purposes:
    * swapping implementations of interfaces
    * libraries
    * sandboxing

    The module system features solve practical problems:
    * selective import -- prevents name collisions and enforces security
    * renaming imports -- resolves name collisions
    * selective export -- prevents dependence on implementation details, enforces security, and improves optimization

    This module system can support incremental compilation.

  * semantics

    * encapsulation

      Modules encapsulate portions of the outermost scope.  Modules compose concatenatively, not by nesting.  Only one instance of a module exists at a time.

      The module system works with /variables/, not values.  If an exported variable is mutated, other modules that import it can read the current value.

    * built-ins

      /All/ variables in this programming language are encapsulated in a module.

      Forms that define and mutate variables, construct and mutate data structures, perform arithmetic and logic, and perform control flow are defined in the "core" module.

      The foreign function interface is defined in the "ffi" module.  Access to the foreign function interface must be restricted to libraries that wrap foreign code for use by this programming language.  Code with access to the foreign function interface could perform harmful output directly or violate the capability security model by converting knowledge of a capability to possession of a capability (by converting a memory address to a reference).

      The internal runtime system interface is defined in the "rsi" module, which will be described later.

      To avoid surprising behavior, user-defined modules cannot be named "core", "ffi", or "rsi".

    * importation, exportation, definition, and mutation

      Multiple, redundant, or unused importations and exportations are /not/ defects so that macros that generate importations or exportations are easier to use.  Importations and exportations return "null".  Optimizing compilers may generate code that evaluates all of a module's importations and exportations, with redundant or unused importations and exportations eliminated, before the rest of the code.

      The importation of a variable must appear before the variable is read in the source code.  Importations are evaluated in topological order and, within that ordering, by the time they are encountered in the source code.  A topological ordering may not be unique, and importations may be evaluated before they are encountered in the source code.  Programming languages without automatic dependency management invariably have an excess of incompatible tools to solve this problem.  Programming languages with predictable load ordering discourage encapsulation by making it easy to use side effects to create invisible dependencies between modules.

      If implicit definitions or importations cause name collisions with each other, "env" has priority over "core".  That is usually the expected and desired behavior.

      If implicit definitions or importations (from "core" or "env") cause name collisions with explicit definitions or importations, explicit definitions and importations have priority.  A warning is issued because name collisions are usually unintentional.

      If explicit definitions or importations cause name collisions with each other, they are normally defects.  Name collisions between explicit definitions or importations normally indicate accidental redefinition.

      Mutating an imported variable is normally a defect because it should not normally be necessary to violate encapsulation.  If the imported variable was overridden by a definition, mutating the variable is /not/ a defect because the variable is in the same module.

      The exportation of a variable may appear before or after the variable is defined or imported in the source code.  Exportations are evaluated by the time the associated module finishes loading.

    * interfaces

      Modules only implicitly import the "core" module and do not implicitly export anything.  The programmer must explicitly specify the interface.  This encourages encapsulation, planning interfaces, and conscious interface change.

      An importation or exportation inside an expression, except for (potentially nested) "do" forms, is a defect.  Module interfaces should not vary at run-time.  Macros may expand into importations and exportations, which makes it possible for module implementations to vary at macro-expansion-time.  Importations and exportations in "do" forms make it possible for a single macro call to expand into a combination of importations, exportations, definitions, and function and macro calls.

      A dependency cycle is a defect.  It indicates two or more modules are badly decomposed.  Some definitions need to be combined into a single module to eliminate the cycle.  Others may need to be separated into other modules.

    * versioning

      Modules may be versioned.  Unversioned modules are usually used for swapping implementations of interfaces.  Versioned modules are usually used for libraries.

      A subset of the Semantic Versioning format is used.  A version number consists of non-negative decimal integers for major, minor, and patch version numbers, in that order, separated by periods.  The major version number is 0 before release, then incremented to 1 at release, and then incremented by 1 each time backwards incompatible changes are made.  The minor version number is incremented by 1 each time new features are included.  The patch version number is incremented by 1 each time defects are corrected.  When the previous number is incremented by 1, the following numbers are reset to 0 (e.g. 1.2.15 to 2.0.0).  Trailing metadata is /not/ supported.

      When loading a versioned module the programming language will use the latest version that meets the specification.  The major version number must be = the requested number, and the minor and patch version numbers must be >= the requested numbers.  The programmer should not request a patch number other than 0 unless they know their implementation is affected by a defect that is corrected in a specific patch version.

    * loading

      "import" causes modules to be executed.

      Even though it is occasionally useful for macro expansion and loading to perform side effects, code /must not/ rely on the time a module is loaded or the number of times a module is macro-expanded or loaded.

      A module may be macro-expanded each time a program that depends on it runs.  Optimizing compilers may generate code that is equivalent to the module's code after macro expansion.

      All of a module's dependencies are recursively reset to their initial states when the module is macro-expanded.  All modules are reset to their initial states when transitioning from macro-expansion-time to run-time.  Modules whose state did not change need not be reset.  The state may be reset by re-executing the macro-expanded code or copying from a backup.  This prevents mutation within a module at macro-expansion-time from influencing the macro expansion of other modules or from influencing run-time.

    * reloading

      Modules can be reloaded at the listener and in modules.  This is useful for exploratory programming or hot code swapping to correct a defect in or add features to a running program.

      "reload" causes modules to be reread and executed.  "reload" is a function that accepts a sequence in the same format as "import"'s arguments and returns "null".

      Reloads are evaluated when they are encountered in the source code.  If a "reload" expression reloads multiple modules, they are reloaded in topological order.

      If a version is specified, this programming language will reload that version.  This makes downgrading possible.  If the specified version fails to reload, it is a defect.  Otherwise, module files are selected for reloading as in normal loading.

      Reloading may have no effect in several situations.  If the same module file would be selected (e.g. by versioning) for reloading and its contents are unchanged, reloading it is unnecessary.  Multiple or redundant reloads are /not/ defects so that macros that generate reloads are easier to use.  If a "reload" expression attempts to reload the same module more than once, it is only reloaded once.  If a "reload" expression specifies different versions of the same module, the highest specified version is reloaded.

      Reloaded modules are macro-expanded with /copies/ of their dependencies that are reset to their initial states.  Reloading a module does /not/ effect the state of other modules at run-time, and the state of other modules at run-time does /not/ effect the macro expansion of a reloaded module.  This avoids surprising behavior by ensuring modules behave the same when initially loaded as when reloaded.

      Attempting to reload a module that is not loaded is a defect.

      Be aware that references to out-of-date values may exist after reloading a module.  You should prefer to access imported values through imported variables.

  * files and paths

    An unversioned module filename consists of the module name.  A versioned module filename consists of the module name and version number, in that order, separated by a hyphen.  The filename extension for source code in this programming language is appended in both cases.

    Only one instance of a module exists at a time, even if there is an unversioned and versioned file, or multiple versioned files, for it.  An unversioned module file is necessary if no version was specified.  A versioned module file that meets the specification is necessary if a version was specified.

    This programming language searches for modules in the directory the program loaded from then a directory for libraries if that fails.

  * conventions

    A user-defined module name consists of symbol names for an (optional) organization, project, and the contents of the module, in that order, separated by periods.

    It is better to be descriptive than terse, to minimize the chance of name collisions.  Using module names is only necessary in importations, so long module names should not annoy programmers.

    For example, "penguin-charity.population-tracker.map" or "population-tracker.map".

* security

  The security problem must be solved at the operating system level, not the programming language level.

  However, this programming language was designed with security in mind.

  * avoid conflation of operating systems and programming languages

    Dan Ingalls, who implemented the first several Smalltalk versions, said: "An operating system is a collection of things that don’t fit into a language.  There shouldn’t be one."

    Conflating an operating system with a programming language prevents composition, encapsulation, and security.

    It prevents reusing the host operating system, if one exists (and it usually does).  The implementer of the programming language is burdened with also implementing an operating system.  The implementation may be inefficient because it differs from the host.

    It prevents changing the operating system's implementation because any software may depend on its implementation details.

    It prevents enforcing security because any software may manipulate the operating system's internals.

  * the security problem

    Programs can do anything the system-wide user or group has permissions to do.

    Malware does not have to reveal its intentions.

    Code injection attacks can cause useful software to behave like malware.

  * how an operating system can solve the security problem

    The solution is the capability security model.

    System-wide users and groups do not exist.  Having access to one service does /not/ imply access to all other services that happen to run on the same computer.

    If the concepts of users or groups are relevant to a service, its implementation includes the service-specific computer security model.  This is already necessary for most services.

    Programs are restricted to the permissions (capabilities) necessary to solve the problem they are intended to solve.

    Programs are granted capabilities two ways.

    When the administrator installs a program they are asked to grant the program's permanent capabilities (e.g. to read its own data files and to read and write its own configuration files).  A program's permanent capabilities are called its "installation endowment".  Installation endowments should be minimal.

    When the user uses a standard system dialog the program is granted a single-use capability to do what the user requested (e.g. to open or save a file).  A dialog that grants single-use capabilities is called a "PowerBox".  Most capabilities should be granted this way.

    Malware is forced to reveal its intentions by asking the administrator to grant an unreasonable installation endowment or by opening unrequested PowerBoxes.

    Code injection attacks are heavily restricted.  Most subverted programs can only damage or transmit their configuration files.  Subverted services can often damage or transmit more valuable data because their capabilities are permanent.

    In all cases software can only be installed, upgraded, and uninstalled by the operating system.  Malware and code injection attacks /cannot/ be used to install other malware.  What is installed is always observable.  Uninstallation is always easy.

    The operating system is observable by the administrator, obedient to the administrator, and forgiving.

  * why an operating system must solve the security problem

    If the capability security model were enforced at the programming language level, it would be easy to bypass by writing software in another programming language.

    Only the operating system can enforce the capability security model on all software.

  * secure programming language design

    Security is effectively encapsulation.

    Therefore, this programming language uses its module system to enforce internal capabilities.  This programming language does not attempt to enforce external capabilities because that would be futile.

    Capabilities are used to implement sandboxed evaluation.

    Sandboxed evaluation is useful for
    * executing untrusted code
    * deserialization
    * genetic programming

    Using the capability security model has other beneficial effects.  Strictly enforced encapsulation encourages better design of this programming language and software written in it.  In theory, software written in this programming language could run on capability-based operating systems, but this programming language would need to be extended to support external capabilities.  In theory, capability-based operating systems could be written in this programming language, but this programming language would need to be extended to support low-level operations.

    Internal capabilities are references.

    Care has been taken to ensure that capabilities can only be obtained by initial conditions (the "core" module is implicitly imported by all modules), construction (e.g. constructing a list), and introduction (being imported, being a free variable, or being passed as an argument).  This is necessary and sufficient to support the capability security model.

    Confinement of capabilities is achieved by sandboxing.

    Revocation of capabilities is achieved by passing a mutable closure that can conditionally operate on the resource, instead of passing the resource directly.

    "sb" is a function that accepts a sequence in the same format as "import"'s arguments and returns a data structure containing, among other things, a sandboxed evaluator.  Only variables in the sandbox's specification can be directly imported inside the sandbox.  "sb" recognizes "env" as a virtual module that contains the environment.  To avoid surprising behavior, user-defined modules cannot be named "env".

    Untrusted code in the anonymous virtual module inside a sandbox is restricted to importing variables in that sandbox's specification, but code in modules that untrusted code imports variables from may import variables not in that sandbox's specification.  For example, it is possible to grant untrusted code access to a function that calls functions from the "ffi" module /without/ granting that untrusted code access to any functions from the "ffi" module.

    Variables from "env" are lexically scoped like other variables.  Looking up variables from "env" does /not/ use the environment chain where the sandboxed evaluator is called.

    Variables from "env" are encapsulated like other variables.  Redefining or mutating variables from "env" does /not/ change their values outside the sandbox.

    A sandboxed evaluator is a function that accepts a form to evaluate and returns the result of evaluation.  Sandboxed evaluators can evaluate any data that is equivalent to source code.

    All variables from the "core" module and any specified variables from the "env" virtual module are imported to the sandbox's environment before evaluation begins.  The form to be evaluated must normally "import" variables from other modules.  Sandboxed evaluators are intended to evaluate normal source code, so normally "import" is necessary.

    Sandboxes compose by nesting.  When a sandbox is constructed, and an outer sandbox exists, the inner sandbox's specification is compared to the outer sandbox's specification.  If the inner sandbox's specification is more permissive, it is a defect.  This normally involves ensuring that the outer sandbox's specification allows directly importing each variable.  Importing variables from "env" is always allowed because lexical scope ensures it cannot become more permissive.

    Using the capability security model has other, less beneficial, effects.  Strictly enforced encapsulation interferes with many desirable activities.  Fortunately, there are workarounds.

    Most problems are effectively wanting to read or mutate variables across module boundaries.  These problems have associated solutions.

    Problem             | Solution
    --------------------+-------------------------------------------------------
    read the variable   | check that the variable can be directly imported
    mutate the variable | cooperation or check that the module can be subverted

    Macro-generated code may contain variables from other modules that the module containing the macro-generated code did not import.  If those variables were always allowed, macros could violate the capability security model.  By checking that those variables can be directly imported, only variables that would violate the capability security model are denied.

    Modules often provide hooks for other modules to attach callbacks to.  This is achieved by providing functions that mutate a data structure within the module containing the hook because it is normally impossible to directly mutate the contents of another module.  This has several beneficial effects.  It is possible to cause side effects when the contents of a hook changes.  Exposing the hook data structure could harm security.

    Code outside a sandbox that can write to a module file can subvert the module.  Code inside a sandbox can only subvert itself.  If code can subvert a module, preventing it from observing or mutating the module's contents is an unnecessary restriction, which this programming language does not enforce.  This works well in practice.  Programmers cannot observe or mutate the contents of built-in modules.  Programmers can observe and mutate the contents of their modules in development.  Encapsulation is strictly enforced in production because production-ready modules are normally read-only.

    Values and call stack frames from variables that can be directly imported and are associated with insubvertible modules can be observed by the debugger.  Values and call stack frames from variables that are associated with subvertible modules can be observed and mutated by the debugger.  If an unhandled error condition occurs in a call stack frame that cannot be observed by the debugger, the stack starts unwinding.  Stack unwinding automatically continues through call stack frames that cannot be observed by the debugger.  The debugger can execute upon reaching a call stack frame that can be observed.  Most programming languages behave similarly because debugging information is usually unavailable for built-in or third-party modules.

    Only module files that can be subverted can be reloaded.  In other words, being allowed to reload a module requires being allowed to mutate its contents.  Programs that use hot code swapping can subvert their reloadable modules.

  * a change to the capability security model

    Violating encapsulation allows software to be used in unintended ways.  Violating encapsulation is not only useful to crackers.  Programmers always consider how to solve the problem their program is intended to automate solving, but they rarely consider how to solve the meta-problem of automating their program.  Violating encapsulation is useful to anyone trying to solve the problem caused by inconsiderate programmers.

    This does raise the question of how to allow violating encapsulation without allowing violating security.  The answer is using external capabilities which requires a capability-based operating system.

    Capability-based operating systems normally associate installation endowments with programs and capabilities with processes (instances of programs).  Capabilities are stored outside the process' address space in a dynamic array called a "C-list".  The process can refer to its capabilities in system calls by integers used to index the C-list, but only the operating system can directly mutate the C-list.

    The separation of capabilities and data gives rise to the term "partitioned capability system".  This has several beneficial effects.  Insecure programming languages like assembly (for most processors), C, and C++ can be used without allowing them to violate security.  Confinement of capabilities (/not/ data) can be achieved without using dynamic or static analysis, by the operating system not transmitting them.  Revocation of capabilities can be achieved without using proxy closures, by the operating system removing them.

    The change I propose is providing the /option/ to install multiple programs in a shared environment.  The installation endowment of the shared environment is the set union of the installation endowments of all programs installed in it.  When multiple programs written in this programming language are installed in a shared environment, their module encapsulation boundaries are not enforced, except inside sandboxes.

    Programs should /not/ be installed in a shared environment by default.

    This programming language's semantics would change in a backwards-compatible way.  Outside sandboxes, modules only indicate which file contains the definition of a variable.  All variables in the outermost scope can be imported.  Imported variables can be mutated by "set!".  All variables and call stack frames can be observed and mutated by the debugger.  All modules can be reloaded.

    This change helps achieve the goal of avoiding unnecessary restrictions.
